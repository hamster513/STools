from fastapi import FastAPI, Request, HTTPException, UploadFile, File, Form
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import os
import json
import uuid
import zipfile
import tarfile
import gzip
import bz2
import lzma
import re
from typing import Dict, List, Optional
from datetime import datetime
import asyncio
from pathlib import Path
import shutil
from database import Database
from models import LogFile, LogSettings, AnalysisPreset
import traceback
from concurrent.futures import ThreadPoolExecutor
import threading

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤
from log_formats import detect_log_level, log_detector

def get_version():
    try:
        with open('VERSION', 'r') as f:
            return f.read().strip()
    except:
        return "0.5.00"

app = FastAPI(title="LogAnalizer", version=get_version())

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
app.mount("/static", StaticFiles(directory="static"), name="static")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
templates = Jinja2Templates(directory="templates")

# –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ä–æ—É—Ç –¥–ª—è CSS —Ñ–∞–π–ª–∞ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
@app.get("/static/css/main.css")
async def get_main_css():
    return FileResponse(
        "../../static/css/main.css",
        media_type="text/css",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0"
        }
    )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
db = Database()

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ñ–∞–π–ª–æ–≤
DATA_DIR = Path("/app/data")
DATA_DIR.mkdir(exist_ok=True)

# –ö—ç—à –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
_cache = {
    'settings': None,
    'presets': None,
    'custom_settings': None
}

# Global dictionary for tracking upload progress
upload_progress = {}

# Global thread pool executor for background tasks
thread_pool = ThreadPoolExecutor(max_workers=4)

# OPTIMIZATION: Add logging for debugging
def log_progress_update(upload_id: str, status: str, message: str, progress: int, details: str = None):
    """Logging progress updates"""
    progress_data = {
        'status': status,
        'message': message,
        'progress': progress,
        'details': details
    }
    upload_progress[upload_id] = progress_data
    print(f"üìä Progress updated for {upload_id}: {progress_data}")

    # OPTIMIZATION: Save to file for debugging
    try:
        with open(f"/app/data/progress_{upload_id}.json", "w") as f:
            json.dump(progress_data, f)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save progress to file: {e}")

def _detect_log_level(line: str, important_levels: List[str]) -> Optional[str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
    """
    return detect_log_level(line, important_levels, debug=True)

@app.on_event("startup")
async def startup():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã PostgreSQL –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–ø—É—Å—Ç–∏–ª—Å—è
        await asyncio.sleep(5)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        await db.init_database()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
        await db.test_connection()
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ
        await _preload_cache()
        
        print("LogAnalizer startup completed successfully")
    except Exception as e:
        print(f"Startup error: {e}")
        raise

async def _preload_cache():
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        _cache['settings'] = await db.get_settings()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ—Å–µ—Ç—ã
        _cache['presets'] = await db.get_presets()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        _cache['custom_settings'] = await db.get_custom_analysis_settings()
        
        print("Cache preloaded successfully")
    except Exception as e:
        print(f"Cache preload error: {e}")

@app.on_event("shutdown")
async def shutdown():
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
    try:
        await db.close()
        print("LogAnalizer shutdown completed")
    except Exception as e:
        print(f"Shutdown error: {e}")

@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    try:
        await db.test_connection()
        return {"status": "healthy", "database": "connected"}
    except Exception as e:
        return {"status": "unhealthy", "database": "disconnected", "error": str(e)}

@app.get("/api/version")
async def get_version():
    """Get application version"""
    try:
        with open('/app/VERSION', 'r') as f:
            version = f.read().strip()
        return {"version": version}
    except Exception as e:
        return {"version": "unknown"}

@app.get("/api/upload-progress/{upload_id}")
async def get_upload_progress(upload_id: str):
    """Get upload progress via SSE"""
    print(f"üîç SSE request for upload_id: {upload_id}")

    async def event_generator():
        while True:
            if upload_id in upload_progress:
                progress_data = upload_progress[upload_id]
                print(f"üì§ Sending SSE data: {progress_data}")
                yield f"data: {json.dumps(progress_data)}\n\n"

                # If upload is completed, stop the stream
                if progress_data.get('status') in ['completed', 'error']:
                    break
            else:
                # If upload_id not found, send an error message
                error_data = {
                    'status': 'not_found',
                    'message': 'Upload ID not found',
                    'progress': 0,
                    'details': f'Upload ID: {upload_id}'
                }
                yield f"data: {json.dumps(error_data)}\n\n"
                break

            await asyncio.sleep(1)

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        }
    )

@app.get("/api/upload-progress-json/{upload_id}")
async def get_upload_progress_json(upload_id: str):
    """Get upload progress in JSON format for fallback"""
    print(f"üîç JSON request for upload_id: {upload_id}")

    if upload_id in upload_progress:
        progress_data = upload_progress[upload_id]
        print(f"üì§ Sending JSON data: {progress_data}")
        return progress_data
    else:
        # If upload_id not found, return an error message
        error_data = {
            'status': 'not_found',
            'message': 'Upload ID not found',
            'progress': 0,
            'details': f'Upload ID: {upload_id}'
        }
        return error_data

@app.get("/api/settings")
async def get_settings():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        # –ï—Å–ª–∏ –∫—ç—à –ø—É—Å—Ç–æ–π –∏–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        if _cache['settings'] is None:
            print("üîÑ Loading settings from database (cache was None)")
            _cache['settings'] = await db.get_settings()
            print(f"üìä Settings loaded: {_cache['settings']}")
        return {"success": True, "data": _cache['settings']}
    except Exception as e:
        print(f"‚ùå Error loading settings: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/settings")
async def update_settings(request: Request):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫—ç—à–∞"""
    try:
        data = await request.json()
        await db.update_settings(data)
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
        _cache['settings'] = None
        return {"success": True, "message": "Settings updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/logs/upload")
async def upload_logs(
    file: UploadFile = File(...),
    max_file_size: int = Form(100),  # MB
    extract_nested: bool = Form(True),
    max_depth: int = Form(5)
):
    """Upload and process log files - OPTIMIZED VERSION WITH BACKGROUND PROCESSING"""
    upload_id = str(uuid.uuid4())
    file_id = str(uuid.uuid4())

    print(f"üöÄ Starting upload: {file.filename} (upload_id: {upload_id})")

    try:
        # Check file size
        if file.size > max_file_size * 1024 * 1024:
            raise HTTPException(status_code=400, detail=f"File too large. Max size: {max_file_size}MB")

        # Save file
        original_filename = file.filename
        file_extension = Path(original_filename).suffix.lower()
        file_path = DATA_DIR / f"{file_id}{file_extension}"

        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        print(f"üíæ File saved: {file_path}")

        # Initialize progress immediately
        log_progress_update(upload_id, 'starting', '–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞...', 0, f'–§–∞–π–ª: {original_filename}')

        # OPTIMIZATION: Return upload_id immediately, processing in background
        result = {
            "success": True,
            "file_id": file_id,
            "upload_id": upload_id,
            "extracted_count": 0,  # Will be updated in background
            "message": f"File uploaded successfully, processing in background"
        }

        print(f"‚úÖ Returning upload_id immediately: {upload_id}")

        # Run processing in thread pool
        def run_processing():
            try:
                print(f"üîÑ Thread processing started for upload_id: {upload_id}")
                # Create new event loop for the thread
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                # Run the async function
                loop.run_until_complete(process_upload_in_background(
                    file_path, file_id, upload_id, original_filename,
                    file_extension, extract_nested, max_depth
                ))
                print(f"‚úÖ Thread processing completed for upload_id: {upload_id}")
            except Exception as e:
                print(f"‚ùå Thread processing error: {e}")
                print(f"‚ùå Error details: {traceback.format_exc()}")
            finally:
                loop.close()
        
        print(f"üéØ Submitting background task to thread pool for upload_id: {upload_id}")
        thread_pool.submit(run_processing)
        print(f"üéØ Background task submitted to thread pool")
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ upload_id –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω
        await asyncio.sleep(0.1)

        return result

    except Exception as e:
        print(f"‚ùå Upload error: {e}")
        log_progress_update(upload_id, 'error', '–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞', 0, str(e))
        raise HTTPException(status_code=500, detail=str(e))

async def process_upload_in_background(
    file_path: Path, file_id: str, upload_id: str,
    original_filename: str, file_extension: str,
    extract_nested: bool, max_depth: int
):
    """Background processing of the uploaded file"""
    # Create new database connection for this thread
    thread_db = Database()
    
    try:
        print(f"üîÑ Starting background processing for upload_id: {upload_id}")
        print(f"üìÅ File path: {file_path}")
        print(f"üÜî File ID: {file_id}")
        print(f"üìÑ Original filename: {original_filename}")
        print(f"üîß Extract nested: {extract_nested}")
        print(f"üìè Max depth: {max_depth}")

        # Initialize database connection
        await thread_db.init_database()

        # STAGE 1: ARCHIVE EXTRACTION
        log_progress_update(upload_id, 'extracting', '–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞...', 10, f'–§–∞–π–ª: {original_filename}')

        extracted_files = []
        if file_path.suffix.lower() in ['.zip', '.tar', '.gz', '.bz2', '.xz']:
            # Add timeout for large archives
            try:
                extracted_files = await asyncio.wait_for(
                    extract_archive(file_path, file_id, extract_nested, max_depth, 0, file_id),
                    timeout=300  # 5 minutes timeout
                )
                print(f"üì¶ Extracted {len(extracted_files)} files")
            except asyncio.TimeoutError:
                log_progress_update(upload_id, 'error', '–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ –∞—Ä—Ö–∏–≤–∞', 0, '–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏')
                return
        else:
            # Regular file
            extracted_files = [{
                'id': file_id,
                'original_name': original_filename,
                'file_path': str(file_path),
                'file_type': file_extension,
                'file_size': file_path.stat().st_size,
                'upload_date': datetime.now(),
                'parent_file_id': None
            }]

        # STAGE 2: SAVING TO DATABASE
        log_progress_update(upload_id, 'saving_to_db', '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...', 50, f'–§–∞–π–ª–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {len(extracted_files)}')

        if len(extracted_files) > 1:
            # Batch insert for archives
            await thread_db.insert_log_files_batch(extracted_files)
        else:
            # Single insert
            await thread_db.insert_log_file(extracted_files[0])

        log_progress_update(upload_id, 'saving_completed', '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ', 70, f'–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(extracted_files)}')

        # STAGE 3: FILE FILTERING
        log_progress_update(upload_id, 'filtering', '–ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ñ–∞–π–ª–æ–≤...', 80, f'–§–∞–π–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(extracted_files)}')

        # OPTIMIZATION: Batch file filtering
        batch_size = 5  # Filter 5 files simultaneously
        total_batches = (len(extracted_files) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(extracted_files))
            batch = extracted_files[start_idx:end_idx]

            # Update progress for the batch
            progress = 80 + (batch_idx + 1) * 15 / total_batches
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä–≤–æ–º —Ñ–∞–π–ª–µ –≤ –ø–∞–∫–µ—Ç–µ
            if batch:
                first_file = batch[0]
                file_size_mb = first_file.get('file_size', 0) / (1024 * 1024)
                log_progress_update(
                    upload_id,
                    'filtering',
                    f'–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ {batch_idx + 1} –∏–∑ {total_batches}...',
                    min(progress, 95),
                    f'–§–∞–π–ª—ã {start_idx + 1}-{end_idx} –∏–∑ {len(extracted_files)} | –¢–µ–∫—É—â–∏–π: {first_file["original_name"]} ({file_size_mb:.1f} MB)'
                )
            else:
                log_progress_update(
                    upload_id,
                    'filtering',
                    f'–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ {batch_idx + 1} –∏–∑ {total_batches}...',
                    min(progress, 95),
                    f'–§–∞–π–ª—ã {start_idx + 1}-{end_idx} –∏–∑ {len(extracted_files)}'
                )
            print(f"üìä Progress updated (filtering batch {batch_idx + 1}): {upload_progress[upload_id]}")

            # OPTIMIZATION: Parallel filtering of the batch with individual file progress
            async def filter_file_parallel(file_info):
                try:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º —Ñ–∞–π–ª–µ
                    file_size_mb = file_info.get('file_size', 0) / (1024 * 1024)
                    log_progress_update(
                        upload_id,
                        'filtering_file',
                        f'–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–∞: {file_info["original_name"]}',
                        progress,
                        f'–†–∞–∑–º–µ—Ä: {file_size_mb:.1f} MB | –ü–∞–∫–µ—Ç {batch_idx + 1} –∏–∑ {total_batches}'
                    )
                    
                    print(f"üîç Starting to filter file: {file_info['original_name']}")
                    result = await filter_log_file(Path(file_info['file_path']), file_info['id'], thread_db)
                    print(f"‚úÖ Successfully filtered file: {file_info['original_name']}")
                    return result
                except Exception as e:
                    print(f"‚ùå Error filtering file {file_info['original_name']}: {e}")
                    print(f"‚ùå Error details: {traceback.format_exc()}")
                    raise

            # Perform parallel filtering of the batch
            print(f"üîÑ Starting parallel filtering of batch {batch_idx + 1} with {len(batch)} files")
            await asyncio.gather(*[filter_file_parallel(file_info) for file_info in batch])
            print(f"‚úÖ Filtered batch {batch_idx + 1}: {[f['original_name'] for f in batch]}")

        # Filtering completion
        log_progress_update(upload_id, 'filtering_completed', '–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞', 95, f'–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(extracted_files)}')

        # Delete original archive file
        if file_path.exists() and file_path.suffix.lower() in ['.zip', '.tar', '.gz', '.bz2', '.xz']:
            file_path.unlink()
            print(f"üóëÔ∏è Original archive deleted")

        # Complete progress
        log_progress_update(upload_id, 'completed', '–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!', 100, f'–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(extracted_files)}')

    except Exception as e:
        print(f"‚ùå Background processing error: {e}")
        print(f"‚ùå Error details: {traceback.format_exc()}")
        log_progress_update(upload_id, 'error', '–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞', 0, str(e))
    finally:
        # Close database connection
        if thread_db.pool:
            await thread_db.close()

async def extract_archive(file_path: Path, file_id: str, extract_nested: bool, max_depth: int, current_depth: int = 0, parent_id: str = None) -> List[Dict]:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤"""
    extracted_files = []
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞—Ä—Ö–∏–≤–∞
        if file_path.suffix.lower() in ['.zip', '.tar', '.gz', '.bz2', '.xz']:
            # –≠—Ç–æ –∞—Ä—Ö–∏–≤, —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
            if current_depth >= max_depth:
                return extracted_files
            
            # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∞—Ä—Ö–∏–≤—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î, —Ç–æ–ª—å–∫–æ –∫–æ–Ω–µ—á–Ω—ã–µ —Ñ–∞–π–ª—ã
            
            extract_dir = DATA_DIR / f"{file_id}_extracted"
            extract_dir.mkdir(exist_ok=True)
            
            if file_path.suffix.lower() == '.zip':
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
            elif file_path.suffix.lower() == '.tar':
                with tarfile.open(file_path, 'r:*') as tar_ref:
                    tar_ref.extractall(extract_dir)
            elif file_path.suffix.lower() == '.gz':
                with gzip.open(file_path, 'rb') as gz_ref:
                    with open(extract_dir / file_path.stem, 'wb') as f:
                        shutil.copyfileobj(gz_ref, f)
            elif file_path.suffix.lower() == '.bz2':
                with bz2.open(file_path, 'rb') as bz2_ref:
                    with open(extract_dir / file_path.stem, 'wb') as f:
                        shutil.copyfileobj(bz2_ref, f)
            elif file_path.suffix.lower() == '.xz':
                with lzma.open(file_path, 'rb') as xz_ref:
                    with open(extract_dir / file_path.stem, 'wb') as f:
                        shutil.copyfileobj(xz_ref, f)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            for extracted_file in extract_dir.rglob('*'):
                if extracted_file.is_file():
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã macOS
                    if extracted_file.name.startswith('._') or extracted_file.name.startswith('.DS_Store'):
                        continue
                    if extract_nested and extracted_file.suffix.lower() in ['.zip', '.tar', '.gz', '.bz2', '.xz']:
                        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏–≤—ã
                        nested_file_id = str(uuid.uuid4())
                        nested_files = await extract_archive(
                            extracted_file, 
                            nested_file_id, 
                            extract_nested, 
                            max_depth, 
                            current_depth + 1,
                            parent_id  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π parent_id
                        )
                        extracted_files.extend(nested_files)
                    else:
                        # –û–±—ã—á–Ω—ã–π —Ñ–∞–π–ª
                        extracted_files.append({
                            'id': str(uuid.uuid4()),
                            'original_name': extracted_file.name,
                            'file_path': str(extracted_file),
                            'file_type': extracted_file.suffix.lower(),
                            'file_size': extracted_file.stat().st_size,
                            'upload_date': datetime.now(),
                            'parent_file_id': None  # –ù–µ —Å—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ –∫–æ—Ä–Ω–µ–≤–æ–π –∞—Ä—Ö–∏–≤
                        })
        else:
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª - –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ extracted_files, 
            # —Ç–∞–∫ –∫–∞–∫ –æ–Ω –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ upload_logs
            pass
    
    except Exception as e:
        print(f"Error extracting archive {file_path}: {e}")
    
    return extracted_files

async def filter_log_file(file_path: Path, file_id: str, db_instance: Database = None, skip_size_check: bool = False) -> Optional[Dict]:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ª–æ–≥ —Ñ–∞–π–ª–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"""
    print(f"üîç Starting filter_log_file for: {file_path}")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ
        db_conn = db_instance if db_instance else db
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        settings = await db_conn.get_settings()
        custom_settings = await db_conn.get_custom_analysis_settings()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        if not skip_size_check:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            max_filtering_size = settings.get('max_filtering_file_size_mb', 50)
            
            if file_size_mb > max_filtering_size:
                print(f"‚ö†Ô∏è File {file_path} is too large for automatic filtering ({file_size_mb:.1f}MB > {max_filtering_size}MB)")
                return None
        else:
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"üîç Manual filtering: File size is {file_size_mb:.1f}MB (size check skipped)")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –ª–æ–≥–æ–≤
        important_levels = settings.get('important_log_levels', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        for setting in custom_settings:
            if setting.get('enabled', True):
                patterns.append(setting['pattern'])
        
        if not important_levels and not patterns:
            return None
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
        filtered_file_path = DATA_DIR / f"{file_id}_filtered"
        filtered_lines = []
        
        # –ß–∏—Ç–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if line:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–Ω–∏ –ª–æ–≥–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
                    if important_levels:
                        detected_level = _detect_log_level(line, important_levels)
                        if detected_level:
                            filtered_lines.append(line)
                            continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                    for pattern in patterns:
                        if re.search(pattern, line):
                            filtered_lines.append(line)
                            break
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
        if filtered_lines:
            with open(filtered_file_path, 'w', encoding='utf-8') as f:
                for line in filtered_lines:
                    f.write(line + '\n')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑—É
            filtered_file_data = {
                'id': str(uuid.uuid4()),
                'original_file_id': file_id,
                'filtered_file_path': str(filtered_file_path),
                'filter_settings': {
                    'important_levels': important_levels,
                    'custom_patterns': [s['name'] for s in custom_settings if s.get('enabled', True)]
                },
                'lines_count': len(filtered_lines)
            }
            
            print(f"üíæ Saving filtered file data to database: {filtered_file_data}")
            try:
                await db_conn.insert_filtered_file(filtered_file_data)
                print(f"‚úÖ Successfully filtered file {file_path}: {len(filtered_lines)} lines")
                return filtered_file_data
            except Exception as e:
                print(f"‚ùå Error saving filtered file data: {e}")
                print(f"‚ùå Error details: {traceback.format_exc()}")
                return None
        
        print(f"‚ö†Ô∏è No matching lines found in file {file_path}")
        return None
        
    except Exception as e:
        print(f"‚ùå Error filtering file {file_path}: {e}")
        print(f"‚ùå Error details: {traceback.format_exc()}")
        return None

@app.get("/api/logs/files")
async def get_log_files():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        files = await db.get_log_files()
        return {"success": True, "data": files}
    except Exception as e:
        print(f"‚ùå Error in get_log_files: {e}")
        print(f"‚ùå Error details: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/logs/files/{file_id}/preview")
async def preview_log_file(file_id: str, lines: int = 100):
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Ñ–∞–π–ª–∞"""
    try:
        file_info = await db.get_log_file(file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")
        
        file_path = Path(file_info['file_path'])
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="File not found on disk")
        
        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ N —Å—Ç—Ä–æ–∫
        preview_lines = []
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for i, line in enumerate(f):
                if i >= lines:
                    break
                preview_lines.append(line.rstrip())
        
        return {
            "success": True,
            "file_id": file_id,
            "original_name": file_info['original_name'],
            "preview": preview_lines,
            "total_lines": len(preview_lines)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/logs/files/{file_id}/filtered")
async def preview_filtered_file(file_id: str, lines: int = 100):
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    try:
        print(f"üîç Requesting filtered file for file_id: {file_id}")
        
        filtered_file_info = await db.get_filtered_file(file_id)
        print(f"üìä Filtered file info: {filtered_file_info}")
        
        if not filtered_file_info:
            print(f"‚ùå No filtered file found for file_id: {file_id}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏
            return {
                "success": True,
                "file_id": file_id,
                "filtered_file_id": None,
                "preview": [],
                "total_lines": 0,
                "filter_settings": {},
                "message": "No matching lines found in file"
            }
        
        file_path = Path(filtered_file_info['filtered_file_path'])
        print(f"üìÅ Checking file path: {file_path}")
        
        if not file_path.exists():
            print(f"‚ùå Filtered file not found on disk: {file_path}")
            raise HTTPException(status_code=404, detail="Filtered file not found on disk")
        
        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ N —Å—Ç—Ä–æ–∫
        preview_lines = []
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for i, line in enumerate(f):
                if i >= lines:
                    break
                preview_lines.append(line.rstrip())
        
        print(f"‚úÖ Successfully read {len(preview_lines)} lines from filtered file")
        
        return {
            "success": True,
            "file_id": file_id,
            "filtered_file_id": filtered_file_info['id'],
            "preview": preview_lines,
            "total_lines": filtered_file_info['lines_count'],
            "filter_settings": filtered_file_info['filter_settings']
        }
        
    except Exception as e:
        print(f"‚ùå Error in preview_filtered_file: {e}")
        print(f"‚ùå Error details: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/logs/files/{file_id}")
async def delete_log_file(file_id: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
    try:
        file_info = await db.get_log_file(file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")
        
        # –£–¥–∞–ª—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –µ—Å—Ç—å
        filtered_file_info = await db.get_filtered_file(file_id)
        if filtered_file_info:
            filtered_file_path = Path(filtered_file_info['filtered_file_path'])
            if filtered_file_path.exists():
                filtered_file_path.unlink()
            await db.delete_filtered_file(file_id)
        
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞
        file_path = Path(file_info['file_path'])
        if file_path.exists():
            file_path.unlink()
        
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        await db.delete_log_file(file_id)
        
        return {"success": True, "message": "File deleted successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/logs/files/clear")
async def clear_all_log_files():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        files = await db.get_log_files()
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å –¥–∏—Å–∫–∞
        for file_info in files:
            file_path = Path(file_info['file_path'])
            if file_path.exists():
                file_path.unlink()
        
        # –û—á–∏—â–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        await db.clear_log_files()
        
        return {"success": True, "message": "All files cleared successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/logs/analyze")
async def analyze_logs(request: Request):
    """–ê–Ω–∞–ª–∏–∑ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    try:
        data = await request.json()
        file_ids = data.get('file_ids', [])
        system_name = data.get('system_name', 'Unknown System')
        preset_id = data.get('preset_id')
        
        if not file_ids:
            raise HTTPException(status_code=400, detail="No files selected for analysis")
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        settings = await db.get_settings()
        important_levels = settings.get('important_log_levels', ['ERROR', 'WARN', 'CRITICAL', 'FATAL', 'ALERT', 'EMERGENCY', 'INFO', 'DEBUG'])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        analysis_results = []
        for file_id in file_ids:
            file_info = await db.get_log_file(file_id)
            if not file_info:
                continue
            
            file_path = Path(file_info['file_path'])
            if not file_path.exists():
                continue
            
            # –ß–∏—Ç–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            important_lines = []
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∞
                    detected_level = self._detect_log_level(line, important_levels)
                    if detected_level:
                        important_lines.append({
                            'line_number': line_num,
                            'content': line.rstrip(),
                            'level': detected_level
                        })
            
            analysis_results.append({
                'file_id': file_id,
                'original_name': file_info['original_name'],
                'important_lines': important_lines,
                'total_lines': len(important_lines)
            })
        
        return {
            "success": True,
            "system_name": system_name,
            "preset_id": preset_id,
            "results": analysis_results
        }
        
    except Exception as e:
        print('Log analysis error:', traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/presets")
async def get_presets():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        if _cache['presets'] is None:
            _cache['presets'] = await db.get_presets()
        return {"success": True, "data": _cache['presets']}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/presets")
async def create_preset(request: Request):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞ —Å –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫—ç—à–∞"""
    try:
        data = await request.json()
        preset_id = await db.create_preset(data)
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
        _cache['presets'] = None
        return {"success": True, "preset_id": preset_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/custom-analysis-settings")
async def get_custom_analysis_settings():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        if _cache['custom_settings'] is None:
            _cache['custom_settings'] = await db.get_custom_analysis_settings()
        return {"success": True, "data": _cache['custom_settings']}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/custom-analysis-settings")
async def create_custom_analysis_setting(request: Request):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        data = await request.json()
        setting_id = await db.create_custom_analysis_setting(data)
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        _cache['custom_settings'] = None
        return {"success": True, "setting_id": setting_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/custom-analysis-settings/{setting_id}")
async def update_custom_analysis_setting(setting_id: str, request: Request):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        data = await request.json()
        await db.update_custom_analysis_setting(setting_id, data)
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        _cache['custom_settings'] = None
        return {"success": True, "message": "Setting updated successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/custom-analysis-settings/{setting_id}")
async def delete_custom_analysis_setting(setting_id: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        await db.delete_custom_analysis_setting(setting_id)
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
        _cache['custom_settings'] = None
        return {"success": True, "message": "Setting deleted successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/log-formats")
async def get_supported_log_formats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ª–æ–≥–æ–≤"""
    try:
        formats = log_detector.get_supported_formats()
        return {"success": True, "data": formats}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/logs/files/{file_id}/filter")
async def filter_existing_file(file_id: str):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞"""
    try:
        file_info = await db.get_log_file(file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")
        
        file_path = Path(file_info['file_path'])
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="File not found on disk")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è —Ä—É—á–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        filtered_result = await filter_log_file(file_path, file_id, db, skip_size_check=True)
        
        if filtered_result:
            return {
                "success": True,
                "message": f"File filtered successfully. Found {filtered_result['lines_count']} matching lines",
                "filtered_file_id": filtered_result['id']
            }
        else:
            return {
                "success": True,
                "message": "No matching lines found in file",
                "filtered_file_id": None
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 